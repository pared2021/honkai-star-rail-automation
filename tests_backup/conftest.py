"""pytesté…ç½®æ–‡ä»¶

å…¨å±€æµ‹è¯•é…ç½®å’Œfixtureå®šä¹‰ã€‚
"""

import pytest
import asyncio
import tempfile
import os
from typing import Generator, AsyncGenerator

# é…ç½®asyncioäº‹ä»¶å¾ªç¯
@pytest.fixture(scope="session")
def event_loop():
    """åˆ›å»ºäº‹ä»¶å¾ªç¯ç”¨äºæ•´ä¸ªæµ‹è¯•ä¼šè¯"""
    import threading
    import signal
    import time
    
    # è®¾ç½®äº‹ä»¶å¾ªç¯ç­–ç•¥
    if os.name == 'nt':  # Windows
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    try:
        yield loop
    finally:
        # å¼ºåˆ¶æ¸…ç†æ‰€æœ‰èµ„æº
        try:
            # 1. å–æ¶ˆæ‰€æœ‰å¼‚æ­¥ä»»åŠ¡
            pending = asyncio.all_tasks(loop)
            if pending:
                for task in pending:
                    if not task.done():
                        task.cancel()
                
                # ç­‰å¾…ä»»åŠ¡å–æ¶ˆï¼Œè®¾ç½®è¶…æ—¶
                try:
                    loop.run_until_complete(
                        asyncio.wait_for(
                            asyncio.gather(*pending, return_exceptions=True),
                            timeout=5.0
                        )
                    )
                except asyncio.TimeoutError:
                    print("è­¦å‘Š: æŸäº›å¼‚æ­¥ä»»åŠ¡æœªèƒ½åœ¨è¶…æ—¶æ—¶é—´å†…å–æ¶ˆ")
            
            # 2. å¼ºåˆ¶åœæ­¢äº‹ä»¶å¾ªç¯
            if loop.is_running():
                loop.stop()
            
            # 3. ç­‰å¾…å¾ªç¯åœæ­¢
            timeout = 3.0
            start_time = time.time()
            while loop.is_running() and (time.time() - start_time) < timeout:
                time.sleep(0.1)
            
            # 4. å…³é—­äº‹ä»¶å¾ªç¯
            if not loop.is_closed():
                loop.close()
                
        except Exception as e:
            print(f"äº‹ä»¶å¾ªç¯æ¸…ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
        # 5. å¼ºåˆ¶æ¸…ç†æ‰€æœ‰æ´»åŠ¨çº¿ç¨‹ï¼ˆé™¤ä¸»çº¿ç¨‹å¤–ï¼‰
        try:
            main_thread = threading.main_thread()
            for thread in threading.enumerate():
                if thread != main_thread and thread.is_alive():
                    print(f"å¼ºåˆ¶ç»ˆæ­¢çº¿ç¨‹: {thread.name}")
                    # å¯¹äºdaemonçº¿ç¨‹ï¼Œè®¾ç½®ä¸ºdaemonä»¥ä¾¿ç¨‹åºé€€å‡ºæ—¶è‡ªåŠ¨ç»ˆæ­¢
                    if hasattr(thread, '_stop'):
                        thread._stop()
        except Exception as e:
            print(f"çº¿ç¨‹æ¸…ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
        # 6. å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        # 7. æœ€åçš„æ¸…ç†
        try:
            # é‡ç½®asyncioçŠ¶æ€
            asyncio.set_event_loop(None)
        except Exception:
            pass


@pytest.fixture
def temp_db_path() -> Generator[str, None, None]:
    """åˆ›å»ºä¸´æ—¶æ•°æ®åº“æ–‡ä»¶è·¯å¾„"""
    db_file = tempfile.NamedTemporaryFile(delete=False, suffix='.db')
    db_path = db_file.name
    db_file.close()
    
    yield db_path
    
    # æ¸…ç†
    if os.path.exists(db_path):
        os.unlink(db_path)


@pytest.fixture
def temp_dir() -> Generator[str, None, None]:
    """åˆ›å»ºä¸´æ—¶ç›®å½•"""
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    
    # æ¸…ç†
    import shutil
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)


# æµ‹è¯•æ ‡è®°
pytest_plugins = []

# è‡ªå®šä¹‰æ ‡è®°
def pytest_configure(config):
    """é…ç½®è‡ªå®šä¹‰æ ‡è®°"""
    config.addinivalue_line(
        "markers", "unit: æ ‡è®°å•å…ƒæµ‹è¯•"
    )
    config.addinivalue_line(
        "markers", "integration: æ ‡è®°é›†æˆæµ‹è¯•"
    )
    config.addinivalue_line(
        "markers", "performance: æ ‡è®°æ€§èƒ½æµ‹è¯•"
    )
    config.addinivalue_line(
        "markers", "slow: æ ‡è®°æ…¢é€Ÿæµ‹è¯•"
    )
    config.addinivalue_line(
        "markers", "database: æ ‡è®°éœ€è¦æ•°æ®åº“çš„æµ‹è¯•"
    )
    config.addinivalue_line(
        "markers", "automation: æ ‡è®°è‡ªåŠ¨åŒ–ç›¸å…³æµ‹è¯•"
    )


# æµ‹è¯•æ”¶é›†é…ç½®
def pytest_collection_modifyitems(config, items):
    """ä¿®æ”¹æµ‹è¯•æ”¶é›†"""
    # ä¸ºæ…¢é€Ÿæµ‹è¯•æ·»åŠ æ ‡è®°
    for item in items:
        if "performance" in item.nodeid:
            item.add_marker(pytest.mark.slow)
        if "integration" in item.nodeid:
            item.add_marker(pytest.mark.integration)
        if "test_automation" in item.nodeid or "automation" in item.nodeid:
            item.add_marker(pytest.mark.automation)
        if any(keyword in item.nodeid for keyword in ["repository", "database", "db"]):
            item.add_marker(pytest.mark.database)


# å¼‚æ­¥æµ‹è¯•æ”¯æŒ
@pytest.fixture(scope="function")
async def async_test_client():
    """å¼‚æ­¥æµ‹è¯•å®¢æˆ·ç«¯"""
    # è¿™é‡Œå¯ä»¥æ·»åŠ å¼‚æ­¥æµ‹è¯•å®¢æˆ·ç«¯çš„è®¾ç½®
    yield


# æ—¥å¿—é…ç½®
@pytest.fixture(autouse=True)
def configure_logging(caplog):
    """é…ç½®æµ‹è¯•æ—¥å¿—"""
    import logging
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    caplog.set_level(logging.INFO)
    
    # é…ç½®ç‰¹å®šæ¨¡å—çš„æ—¥å¿—çº§åˆ«
    logging.getLogger("src.repositories").setLevel(logging.WARNING)
    logging.getLogger("src.core").setLevel(logging.INFO)
    logging.getLogger("src.services").setLevel(logging.INFO)


# æµ‹è¯•æ•°æ®æ¸…ç†
@pytest.fixture(autouse=True)
def cleanup_test_data():
    """è‡ªåŠ¨æ¸…ç†æµ‹è¯•æ•°æ®"""
    yield
    
    # æµ‹è¯•åæ¸…ç†
    import gc
    gc.collect()


# å¼ºåˆ¶è¿›ç¨‹æ¸…ç†
@pytest.fixture(autouse=True, scope="function")
def force_cleanup_processes():
    """å¼ºåˆ¶æ¸…ç†è¿›ç¨‹å’Œèµ„æº"""
    import psutil
    import threading
    import time
    
    # è®°å½•æµ‹è¯•å¼€å§‹æ—¶çš„è¿›ç¨‹å’Œçº¿ç¨‹çŠ¶æ€
    initial_threads = set(threading.enumerate())
    current_process = psutil.Process()
    initial_children = current_process.children(recursive=True)
    
    yield
    
    # æµ‹è¯•ç»“æŸåå¼ºåˆ¶æ¸…ç†
    try:
        # 1. æ¸…ç†å­è¿›ç¨‹
        current_children = current_process.children(recursive=True)
        new_children = [p for p in current_children if p not in initial_children]
        
        for child in new_children:
            try:
                print(f"ç»ˆæ­¢å­è¿›ç¨‹: {child.pid} - {child.name()}")
                child.terminate()
                child.wait(timeout=3)
            except (psutil.NoSuchProcess, psutil.TimeoutExpired):
                try:
                    child.kill()
                except psutil.NoSuchProcess:
                    pass
            except Exception as e:
                print(f"æ¸…ç†å­è¿›ç¨‹æ—¶å‡ºé”™: {e}")
        
        # 2. æ¸…ç†æ–°åˆ›å»ºçš„çº¿ç¨‹
        current_threads = set(threading.enumerate())
        new_threads = current_threads - initial_threads
        
        for thread in new_threads:
            if thread.is_alive() and thread != threading.current_thread():
                try:
                    print(f"ç­‰å¾…çº¿ç¨‹ç»“æŸ: {thread.name}")
                    thread.join(timeout=2)
                    if thread.is_alive():
                        print(f"çº¿ç¨‹ {thread.name} æœªèƒ½æ­£å¸¸ç»“æŸ")
                        # å¯¹äºdaemonçº¿ç¨‹ï¼Œå¼ºåˆ¶è®¾ç½®ä¸ºdaemon
                        if not thread.daemon:
                            thread.daemon = True
                except Exception as e:
                    print(f"æ¸…ç†çº¿ç¨‹æ—¶å‡ºé”™: {e}")
        
        # 3. å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        
        # 4. æ¸…ç†asyncioç›¸å…³èµ„æº
        try:
            loop = asyncio.get_event_loop()
            if loop and not loop.is_closed():
                pending = asyncio.all_tasks(loop)
                for task in pending:
                    if not task.done():
                        task.cancel()
        except RuntimeError:
            # æ²¡æœ‰äº‹ä»¶å¾ªç¯è¿è¡Œ
            pass
        except Exception as e:
            print(f"æ¸…ç†asyncioèµ„æºæ—¶å‡ºé”™: {e}")
            
    except Exception as e:
        print(f"å¼ºåˆ¶æ¸…ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")


# Mocké…ç½®
@pytest.fixture
def mock_automation_operations():
    """Mockè‡ªåŠ¨åŒ–æ“ä½œ"""
    from unittest.mock import Mock, AsyncMock
    from src.services.automation_application_service import WindowInfo, ElementMatch
    
    mocks = {
        'detect_game_window': AsyncMock(return_value=WindowInfo(
            window_id="mock_window",
            title="Mock Game Window",
            rect={'x': 0, 'y': 0, 'width': 1920, 'height': 1080},
            process_id=1234,
            is_active=True
        )),
        'take_screenshot': AsyncMock(return_value="mock_screenshot.png"),
        'find_element': AsyncMock(return_value=ElementMatch(
            element_id="mock_element",
            confidence=0.95,
            position={'x': 100, 'y': 200},
            size={'width': 80, 'height': 30}
        )),
        'click_element': AsyncMock(return_value=True),
        'input_text': AsyncMock(return_value=True),
        'wait_for_element': AsyncMock(return_value=ElementMatch(
            element_id="waited_element",
            confidence=0.90,
            position={'x': 150, 'y': 250},
            size={'width': 100, 'height': 40}
        )),
        'execute_automation_sequence': AsyncMock(return_value=True)
    }
    
    return mocks


# æ€§èƒ½æµ‹è¯•é…ç½®
@pytest.fixture
def performance_config():
    """æ€§èƒ½æµ‹è¯•é…ç½®"""
    return {
        'max_task_creation_time': 0.1,  # æœ€å¤§ä»»åŠ¡åˆ›å»ºæ—¶é—´(ç§’)
        'max_database_operation_time': 0.05,  # æœ€å¤§æ•°æ®åº“æ“ä½œæ—¶é—´(ç§’)
        'min_throughput': 1.0,  # æœ€å°ååé‡(ä»»åŠ¡/ç§’)
        'max_memory_increase': 200,  # æœ€å¤§å†…å­˜å¢é•¿(MB)
        'max_event_processing_time': 0.01,  # æœ€å¤§äº‹ä»¶å¤„ç†æ—¶é—´(ç§’)
    }


# æµ‹è¯•ç¯å¢ƒä¿¡æ¯
@pytest.fixture(scope="session")
def test_environment_info():
    """æµ‹è¯•ç¯å¢ƒä¿¡æ¯"""
    import platform
    import sys
    
    return {
        'platform': platform.platform(),
        'python_version': sys.version,
        'architecture': platform.architecture(),
        'processor': platform.processor(),
    }


# æµ‹è¯•æŠ¥å‘Šé’©å­
def pytest_terminal_summary(terminalreporter, exitstatus, config):
    """æµ‹è¯•ç»“æŸåçš„æ€»ç»“æŠ¥å‘Š"""
    if hasattr(terminalreporter, 'stats'):
        stats = terminalreporter.stats
        
        print("\n" + "="*60)
        print("æµ‹è¯•æ‰§è¡Œæ€»ç»“")
        print("="*60)
        
        if 'passed' in stats:
            print(f"âœ… é€šè¿‡: {len(stats['passed'])} ä¸ªæµ‹è¯•")
        
        if 'failed' in stats:
            print(f"âŒ å¤±è´¥: {len(stats['failed'])} ä¸ªæµ‹è¯•")
        
        if 'skipped' in stats:
            print(f"â­ï¸  è·³è¿‡: {len(stats['skipped'])} ä¸ªæµ‹è¯•")
        
        if 'error' in stats:
            print(f"ğŸ’¥ é”™è¯¯: {len(stats['error'])} ä¸ªæµ‹è¯•")
        
        print("="*60)